{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2-2-learning-rate-optimization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNDuWvvbTCoXiP4AvFYn+OC"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Adnxs5kDc2pd"
      },
      "source": [
        "# (day2) Section 2: 学習率最適化問題\n",
        "\n",
        "本書は、「深層学習前編（day2）レポート」の、「Section 2: 学習率最適化問題」についてのものです。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_jMHYWudCh3"
      },
      "source": [
        "## 1. 要点まとめ\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feL7EoVRgBWN"
      },
      "source": [
        "### 学習率\n",
        "\n",
        "勾配降下法の学習率は、大き過ぎても、小さ過ぎてもいけない。\n",
        "\n",
        "$$\n",
        "\\textbf{w}^{t+1} = \\textbf{w}^{t} - \\epsilon \\nabla E\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\epsilon: 学習率\n",
        "$$\n",
        "\n",
        "- 大きい場合<br>\n",
        "発散\n",
        "\n",
        "- 小さい場合<br>\n",
        "  - 非効率\n",
        "  - 大域的局所最適値てばない、局所最適解に収束\n",
        "\n",
        "よって、学習率を最適化する。最適化する手法は、以下の通り。\n",
        "\n",
        "- モメンタム\n",
        "- AdaGrad\n",
        "- RMSProp\n",
        "- Adam\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIIy0PKCgJvt"
      },
      "source": [
        "### 学習率最適化手法\n",
        "\n",
        "- NOTE: [Jaewan-Yun/optimizer-visualization: Visualize Tensorflow's optimizers.](https://github.com/Jaewan-Yun/optimizer-visualization)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mk3buBqDg9MR"
      },
      "source": [
        "#### モメンタム\n",
        "\n",
        "$$\n",
        "V_t = \\mu V_{t-1} - \\epsilon \\nabla E\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\textbf{w}^{(t+1)} = \\textbf{w}^{(t)} + V_t\n",
        "$$\n",
        "\n",
        "$$\n",
        "慣性: \\mu\n",
        "$$\n",
        "\n",
        "株価のチャートの、移動平均線に近い動きをする。\n",
        "\n",
        "- コード\n",
        "  - `common/optimizer.py`<br>\n",
        "    `class Momentum`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yt0nP2A3hBCp"
      },
      "source": [
        "#### AdaGrad\n",
        "\n",
        "$$\n",
        "\\theta: 何かしらの値の初期値\n",
        "$$\n",
        "\n",
        "$$\n",
        "h: 前回までの勾配の調整量を蓄積した値\n",
        "$$\n",
        "\n",
        "\n",
        "$$\n",
        "h_0 = \\theta\n",
        "$$\n",
        "\n",
        "計算した勾配の2乗を保持。\n",
        "\n",
        "$$\n",
        "h_t = h_{t-1} + ( \\nabla E )^2\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\textbf{w}^{(t+1)}\n",
        "=\n",
        "\\textbf{w}^{(t)}\n",
        "-\n",
        "\\epsilon \\frac{1}\n",
        "{ \\sqrt{h_t} + \\theta }\n",
        "\\nabla E\n",
        "$$\n",
        "\n",
        "- コード\n",
        "  - `common/optimizer.py`<br>\n",
        "    `class AdaGrad`<br>\n",
        "    コードでの $ \\theta $ は、`1e-7`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SU50xGYfuzvS"
      },
      "source": [
        "- NOTE: 鞍点問題\n",
        "  - [DeepLearningがなぜうまく学習出来るのか | ALBERT Official Blog](https://blog.albert2005.co.jp/2016/08/24/deeplearning%E3%81%8C%E3%81%AA%E3%81%9C%E3%81%86%E3%81%BE%E3%81%8F%E5%AD%A6%E7%BF%92%E5%87%BA%E6%9D%A5%E3%82%8B%E3%81%AE%E3%81%8B/)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guihOXsGhE-B"
      },
      "source": [
        "#### RMSProp\n",
        "\n",
        "$$\n",
        "\\alpha: 前回までの勾配情報を h をどのくらい使うか? ( 0 \\leq \\alpha \\leq 1 )\n",
        "$$\n",
        "\n",
        "$$\n",
        "h_t = \\alpha h_{(t-1)} + (1 - \\alpha)(\\nabla E)^2\n",
        "$$\n",
        "\n",
        "上記の計算が AdaGrad との違い。以下の重みの更新は、 AdaGrad と同じ。\n",
        "\n",
        "$$\n",
        "\\textbf{w}^{(t+1)}\n",
        "=\n",
        "\\textbf{w}^{(t)}\n",
        "-\n",
        "\\epsilon \\frac{1}\n",
        "{ \\sqrt{h_t} + \\theta }\n",
        "\\nabla E\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVEWrLi-zqOa"
      },
      "source": [
        "- コード\n",
        "  - `common/optimizer.py`<br>\n",
        "    `class RMSprop`<br>\n",
        "    コードでの $ \\alpha $ は、`decay_rate = 0.99`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWr7dzruhJWf"
      },
      "source": [
        "#### Adam\n",
        "\n",
        "モメンタムと RMSProp を合わせたもの。\n",
        "\n",
        "- コード\n",
        "  - `common/optimizer.py`<br>\n",
        "    `class Adam`<br>\n",
        "\n",
        "\n",
        "コードより、数式は以下の通り。\n",
        "\n",
        "$$\n",
        "\\epsilon_t = \\epsilon \\frac{\\sqrt{1 - \\beta_2^i}}\n",
        "{1 - \\beta_1^i}\n",
        "$$\n",
        "\n",
        "その都度、更新に使用する学習率を計算する。\n",
        "\n",
        "i: イテレーション数\n",
        "\n",
        "$$\n",
        "\\beta_1, \\beta_2\n",
        "$$\n",
        "\n",
        "は、コードでは、 `beta1=0.9, beta2=0.999` 。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_KjHqT-4HD1"
      },
      "source": [
        "$$\n",
        "m_t = m_{t-1} + (1 - \\beta_1)(\\nabla E - m_{t-1})\n",
        "$$\n",
        "\n",
        "$$\n",
        "v_t = v_{t-1} + (1 - \\beta_2)((\\nabla E)^2 - v_{t-1})\n",
        "$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULWuEZEV5dTN"
      },
      "source": [
        "$$\n",
        "\\textbf{w}^{(t+1)}\n",
        "=\n",
        "\\textbf{w}^{(t)}\n",
        "-\n",
        "\\epsilon_{t} \\frac{m_{t}}\n",
        "{ \\sqrt{v_{t}} + \\theta }\n",
        "\\nabla E\n",
        "$$\n",
        "\n",
        "RMSProp に対して、分子の $ m_t $ にモメンタムの要素が追加されている。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dn0nkaSx8-Em"
      },
      "source": [
        "## 2. 確認テスト\n",
        "\n",
        "以降の \"page. \" は、講義資料のページの番号です。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kqh2Tek9hO_E"
      },
      "source": [
        "### Page. 44\n",
        "\n",
        "- モメンタム<br>\n",
        "  SGD に慣性(勢い)を付けたもの。また、株式チャートの移動平均線の様に、一時的な振動を抑制して収束する。\n",
        "\n",
        "- AdaGrad<br>\n",
        "  前回までの勾配の調整量を蓄積した値で、学習率を調整する。\n",
        "\n",
        "- RMSProp<br>\n",
        "  AdaGrad の、前回までの勾配の調整量を、どのくらい使うかを変えられるようにしたもの。\n",
        "\n",
        "- Adam<br>\n",
        "  RMSProp ( と、それのもとになった AdaGrad ) に、モメンタムの慣性を追加したもの。\n"
      ]
    }
  ]
}