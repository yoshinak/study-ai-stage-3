{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1-3-output-layer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMhbgJJFSUb8e2SaTMNS/pm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Adnxs5kDc2pd"
      },
      "source": [
        "# Section 3: 出力層\n",
        "\n",
        "本書は、「深層学習前編（day1）レポート」の、「Section 3: 出力層」についてのものです。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_jMHYWudCh3"
      },
      "source": [
        "## 1. 要点まとめ\n",
        "\n",
        "出力層は、ニューラルネットワークにおいて、欲しい結果を出力する層である。<br>\n",
        "例えば、分類問題の場合、分類した結果が主力層のノード毎に出力される。<br>\n",
        "( 各クラスは確率なので、0 - 1 の範囲、そして、総和は 1 )\n",
        "\n",
        "全体像の例において、出力層にあたる数式は、以下の通り。\n",
        "\n",
        "- 出力と出力層活性化関数<br>\n",
        "$$\n",
        "\\textbf{y}_N^{(t)} = \\textbf{z}^{(2)} = f^{(2)}(\\textbf{u}^{(2)})\n",
        "$$\n",
        "\n",
        "- 誤差関数<br>\n",
        "訓練結果と、訓練データの正解値との、誤差を計算する。<br>\n",
        "$$\n",
        "E_n(\\textbf{w})\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kdW-U7WSFS6"
      },
      "source": [
        "#### 問題による活性化関数と誤差関数の使い分け\n",
        "\n",
        "適用する問題により、以下の関数を使用する。\n",
        "\n",
        "| 関数の種類 | 回帰 | 二値分類 | 他クラス分類 |\n",
        "| --- | --- | --- | --- |\n",
        "| 活性化関数 | 恒等写像 | シグモイド関数 | ソフトマックス関数 |\n",
        "| 誤差関数 | 二乗誤差 | 交差エントロピー | 同左 |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UupTD6UCR-HR"
      },
      "source": [
        "### 3-1 誤差関数\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwgvsQEBf68_"
      },
      "source": [
        "#### 二乗誤差\n",
        "\n",
        "page. 37 の例では、誤差関数に、二乗誤差を使っている。計算式は以下の通り。<br>\n",
        "(分類問題では、通常、交差エントロピーを使うことに注意)\n",
        "\n",
        "$$\n",
        "E_n(\\textbf{w})\n",
        "=\n",
        "\\frac{1}{2}\n",
        "\\sum_{k = 1}^K(\n",
        "  y_k - d_k\n",
        ")^2 \\\\\n",
        "=\n",
        "\\frac{1}{2}\n",
        "\\Vert\n",
        "  \\textbf{y}_k - \\textbf{d}_k\n",
        "\\Vert^2\n",
        "$$\n",
        "<br>\n",
        "( $ k $: 出力層ノードのインデックス )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQnBBYhyRkpP"
      },
      "source": [
        "#### 交差エントロピー\n",
        "\n",
        "数式は以下の通り。\n",
        "\n",
        "$$\n",
        "E_n(\\textbf{w})\n",
        "=\n",
        "- \\sum_{k = 1}^K d_k \\log y_k\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBbJyV34nWiv"
      },
      "source": [
        "また、学習あたり ( 訓練データのインデックス: $ n = 1 \\cdots N $  の $ N $ まで ) の誤差は以下となる。\n",
        "\n",
        "$$\n",
        "E(\\textbf{w}) = \\sum_{n = 1}^N E_n\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nnuf8YFjf_Jz"
      },
      "source": [
        "### 3-2 出力層の活性化関数\n",
        "\n",
        "出力層で使われる活性化関数は、中間層と異なり、信号の大きさ(比率)をそのままに変換する。\n",
        "\n",
        "それぞれの活性化関数の数式は以下の通り。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9sUWpy_ofOr"
      },
      "source": [
        "#### シグモイド関数\n",
        "\n",
        "$$\n",
        "f(u) = \\frac{1}{1 + e^{-u}}\n",
        "$$\n",
        "\n",
        "( $ u $ : 出力層の入力 )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vENout44putX"
      },
      "source": [
        "#### ソフトマックス関数\n",
        "\n",
        "$$\n",
        "f(i, u) = \\frac{e^{u_i}}\n",
        "{ \\sum_{k = 1}^K e^{u_k}}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dn0nkaSx8-Em"
      },
      "source": [
        "## 2. 確認テスト\n",
        "\n",
        "以降の \"page. \" は、講義資料のページの番号です。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgr4yRSJgLdi"
      },
      "source": [
        "### page. 38\n",
        "\n",
        "- 2乗する理由<br>\n",
        "誤差は負の場合もあるので、大きさ(絶対値)を得るため。\n",
        "\n",
        "- 続く勾配降下法で、微分するときに、\n",
        "$ x^2 $ を微分して $ 2 x $ としたら、\n",
        "$ \\frac{1}{2} $ で打ち消せるから。(計算の都合)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEjaHrekgOKz"
      },
      "source": [
        "### page. 45\n",
        "\n",
        "ソフトマックス関数の、 1 - 3 に該当するコードは以下の通り。\n",
        "\n",
        "1. \n",
        "```python\n",
        "return np.exp(x) / np.sum(np.exp(x))\n",
        "```\n",
        "にあたる。\n",
        "\n",
        "1. \n",
        "```python\n",
        "return np.exp(x) / np.sum(np.exp(x))\n",
        "```\n",
        "の、 `np.exp(x)` にあたる。\n",
        "\n",
        "1. \n",
        "```python\n",
        "return np.exp(x) / np.sum(np.exp(x))\n",
        "```\n",
        "の、 `np.sum(np.exp(x))` にあたる。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNjJhuzWgZZj"
      },
      "source": [
        "### page. 47\n",
        "\n",
        "1. \n",
        "```python\n",
        "return -np.sum(np.log(y[np.arange(batch_size), d] + 1e-7)) / batch_size\n",
        "```\n",
        "にあたる。\n",
        "\n",
        "1. \n",
        "```python\n",
        "return -np.sum(np.log(y[np.arange(batch_size), d] + 1e-7)) / batch_size\n",
        "```\n",
        "の、\n",
        "`-np.sum(np.log(y[np.arange(batch_size), d] + 1e-7))` にあたる。\n",
        "\n",
        "- NOTE: 講義のメモ<br>\n",
        "  - 'y': 0, 1\n",
        "  - `1e-7`: 対数関数で 0 の時、 $ - \\infty $ になってしまうの回避するため。\n"
      ]
    }
  ]
}