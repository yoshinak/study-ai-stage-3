{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2-1-vanishing-gradient-problem.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOt6Tl5p6bbj4YQrgdm4GYG"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Adnxs5kDc2pd"
      },
      "source": [
        "# (day2) Section 1: 勾配消失問題\n",
        "\n",
        "本書は、「深層学習前編（day2）レポート」の、「Section 1: 勾配消失問題」についてのものです。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_jMHYWudCh3"
      },
      "source": [
        "## 1. 要点まとめ\n",
        "\n",
        "勾配消失問題は、学習が進むと、勾配が小さくなり、パラメータが更新されないこと。結果、訓練で大域的局所解に収束しない。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWuBL8-Pjitc"
      },
      "source": [
        "シグモイド関数は、大きな値の場合、変化が小さくなる。それで、勾配(変化) が消失してしまう。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIEJ4rsUjVT0"
      },
      "source": [
        "勾配消失を解決する方法は、以下の通り。\n",
        "\n",
        "- 活性化関数\n",
        "\n",
        "- 重みの初期値\n",
        "\n",
        "- バッチ正規化\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGH7gtCCi8LA"
      },
      "source": [
        "### 活性化関数\n",
        "\n",
        "活性化関数をシグモイド関数から、 ReLU 関数に変える。\n",
        "\n",
        "$$\n",
        "\\begin{equation}\n",
        "\\begin{cases}\n",
        "x & ( x \\gt 0 ) \\\\\n",
        "0 & ( x \\leqq 0 )\n",
        "\\end{cases}\n",
        "\\end{equation}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTFBy1ZGA1nz"
      },
      "source": [
        "- NOTE: スパース化<br>\n",
        "  以下を参考にした。<br>\n",
        "  - [東芝レビュー74巻4号（2019年7月）](https://www.global.toshiba/content/dam/toshiba/migration/corp/techReviewAssets/tech/review/2019/04/74_04pdf/f01.pdf)<br>\n",
        "  > スパース（疎）はまばらという意味 であり，機械学習や統計学の分野ではデータの大半がゼロ で，意味のあるものは少数に限られること<br>\n",
        "  ...<br>\n",
        "  > ReLUは入力が 正の場合は恒等写像を，ゼロ以下の場合はゼロを出力する 関数である<br>\n",
        "\n",
        "上記の、 \"ゼロ以下の場合はゼロを出力する\" が、意味のあるものを残すスパース化に貢献していると理解した。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEyYJchyjEuL"
      },
      "source": [
        "### 初期値の設定方法\n",
        "\n",
        "NN の「個性」を与えるため、重み ( $ \\textbf{W} $ ) の初期値の与え方の分布を調整する。<br>\n",
        "重みの分布が 0, 1 に偏ったり、中央値に集中すると、 NN としての学習の意味がないため、正規分布の乱数で初期値を与える。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4bh-iW2t2cZ"
      },
      "source": [
        "#### Xavier\n",
        "\n",
        "```python\n",
        "    # Xavierの初期値\n",
        "    network['W1'] = np.random.randn(input_layer_size, hidden_layer_1_size) / (np.sqrt(input_layer_size))\n",
        "```\n",
        "\n",
        "- [numpy.random.randn — NumPy v1.21 Manual](https://numpy.org/doc/stable/reference/random/generated/numpy.random.randn.html)<br>\n",
        "  > Return a sample (or samples) from the “standard normal” distribution.\n",
        "\n",
        "正規分布で発生させた乱数値を、前層ノード数の平方根で除算。\n",
        "\n",
        "- NOTE: [Xavierの初期値](https://qiita.com/Becon147/items/a9971041bca5c10483bc#xavier%E3%81%AE%E5%88%9D%E6%9C%9F%E5%80%A4)<br>\n",
        "  > 平均0,標準偏差 $ \\frac{1}{\\sqrt{n}} $ である正規分布から初期値を生成\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-J6PHixDjqD"
      },
      "source": [
        "#### He\n",
        "\n",
        "```python\n",
        "    # Heの初期値\n",
        "    network['W1'] = np.random.randn(input_layer_size, hidden_layer_1_size) / np.sqrt(input_layer_size) * np.sqrt(2)\n",
        "```\n",
        "\n",
        "Xavier に $ \\sqrt{2} $ を掛けたもの。\n",
        "\n",
        "- NOTE: [ニューラルネットワークの重みの初期値について解説! | AVILEN AI Trend](https://ai-trend.jp/basic-study/neural-network/initial_value/)<br>\n",
        "  > ReLU関数を活性化関数としたモデルにXavierの初期値を用いるとアクティベーション分布に偏りが生じやすくなる\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJTzF9QcjJcp"
      },
      "source": [
        "### バッチ正規化\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-COzC-vynCjA"
      },
      "source": [
        "#### ミニバッチ単位\n",
        "\n",
        "特にイメージデータの場合、バッチ全体のデータで一度に学習することができない。(メモリー量大)\n",
        "ミニバッチ単位に訓練データを分割する\n",
        "\n",
        "`2_3_batch_normalization.ipynb` の中で対応するコードは以下の通り。訓練データからランダムに 100 件を抽出している。\n",
        "\n",
        "```python\n",
        "batch_size = 100\n",
        "...\n",
        "\n",
        "for i in range(iters_num):\n",
        "    batch_mask = np.random.choice(train_size, batch_size)\n",
        "    x_batch = x_train[batch_mask]\n",
        "    d_batch = d_train[batch_mask]\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gp3wo12SnJev"
      },
      "source": [
        "#### バッチ正規化層\n",
        "\n",
        "ミニバッチ単位で、データの分布を正規化する。\n",
        "学習データの極端な偏りをなくし、過学習を回避できる。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7p8hhKvnyef"
      },
      "source": [
        "#### バッチ正規化の数式\n",
        "\n",
        "最終的に求めたいのは、「4. 変倍・移動」である。\n",
        "\n",
        "ミニバッチ単位で、訓練データ $ x $ の正規化 ( 標準化 ) を行った値で、<br>\n",
        "出力 $ y $ を「変倍・移動」する。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmQii_xxpPRr"
      },
      "source": [
        "##### 1. ミニバッチの平均\n",
        "\n",
        "$$\n",
        "\\mu_t = \\frac{1}{N_t} \\sum_{i=1}^{N_t} x_{ni}\n",
        "$$\n",
        "\n",
        "ミニバッチ単位に含まれる訓練データが、 8 枚の場合、 $ N_t = 8 $ 。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVO9U4Pwp5qi"
      },
      "source": [
        "##### 2. ミニバッチの分散\n",
        "\n",
        "$$\n",
        "\\sigma_t^2 = \\frac{1}{N_t} \\sum_{i=1}^{N_t} (\n",
        "  x_{ni} - \\mu_t\n",
        ")^2\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fadpJipeqvAg"
      },
      "source": [
        "##### 3. ミニバッチの正規化\n",
        "\n",
        "ミニバッチ単位の中で、正規化 ( 標準化 ) された値を得る。\n",
        "\n",
        "$$\n",
        "\\hat{x_{ni}} = \\frac{x_{ni} - \\mu_t}\n",
        "{\\sqrt{\\sigma_t^2 + \\theta}}\n",
        "$$\n",
        "\n",
        "$ \\theta $ はバイアス。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtlWUDnFqaRH"
      },
      "source": [
        "- NOTE: 統計的正規化\n",
        "  - [正規化（Normalization）／標準化（Standardization）とは？：AI・機械学習の用語辞典 - ＠IT](https://atmarkit.itmedia.co.jp/ait/articles/2110/07/news027.html)<br>\n",
        "    > 平均0、分散1にスケーリングする「Z-score normalization」<br>\n",
        "    ...<br>\n",
        "    > Z-score normalizationは、標準化（Standardization）と呼ばれる\n",
        "\n",
        "ここでは、「標準化」 ( 平均0、分散1にスケーリング ) をしている。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMyMUIP8n52v"
      },
      "source": [
        "##### 4. 変倍・移動\n",
        "\n",
        "NN が扱いやすいように、定数倍 ( $ \\gamma $ )、バイアス ( $ \\beta $ ) を加える。\n",
        "\n",
        "$$\n",
        "y_{ni} = \\gamma x_{ni} + \\beta\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFUwKDe9xRay"
      },
      "source": [
        "対応するコードは、以下の通り。\n",
        "\n",
        "- common/layers.py: `class BatchNormalization`<br>\n",
        "  ```pyhon\n",
        "    def __forward(self, x, train_flg):\n",
        "...\n",
        "            mu = x.mean(axis=0)\n",
        "            xc = x - mu\n",
        "            var = np.mean(xc**2, axis=0)\n",
        "            std = np.sqrt(var + 10e-7)\n",
        "            xn = xc / std\n",
        "...\n",
        "        out = self.gamma * xn + self.beta \n",
        "        return out\n",
        "    ```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dn0nkaSx8-Em"
      },
      "source": [
        "## 2. 確認テスト\n",
        "\n",
        "以降の \"page. \" は、講義資料のページの番号です。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsU9aByGhvi4"
      },
      "source": [
        "### page. 10\n",
        "\n",
        "$$\n",
        "\\frac{\\partial{z}}{\\partial{x}}\n",
        "=\n",
        "\\frac{\\partial{z}}{\\partial{t}}\n",
        "\\frac{\\partial{t}}{\\partial{x}}\n",
        "$$\n",
        "\n",
        "$$\n",
        "=\n",
        "2t \\times 1 \\\\\n",
        "=\n",
        "2(x + y)\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k406U1wW2BN5"
      },
      "source": [
        "### page. 18\n",
        "\n",
        "0.25 になる。\n",
        "\n",
        "シグモイド関数の微分は、以下であるから。\n",
        "\n",
        "$$\n",
        "f(x) = \\frac{1}{1 + e^{x}}\n",
        "$$\n",
        "\n",
        "とすると、\n",
        "\n",
        "$$\n",
        "f'(x) = f(x) (1 - f(x))\n",
        "$$\n",
        "\n",
        "$$\n",
        "f(0) = 0.5\n",
        "$$\n",
        "\n",
        "から、\n",
        "\n",
        "$$\n",
        "0.5 \\times (1 - 0.5) = 0.25s\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIKk8g5uFriB"
      },
      "source": [
        "### page. 26\n",
        "\n",
        "重みを 0 に設定すると、それぞれの重みの多様性が失われ、結果、 NN として意味のない重みとなってしまうため。\n",
        "そこから、学習を重ねると非効率であるため。\n"
      ]
    }
  ]
}