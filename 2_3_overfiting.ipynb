{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2-3-overfiting.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNL2F+PFXTBYQGW70T3BQ36"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Adnxs5kDc2pd"
      },
      "source": [
        "# (day2) Section 3: 過学習\n",
        "\n",
        "本書は、「深層学習前編（day2）レポート」の、「Section 3: 過学習」についてのものです。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_jMHYWudCh3"
      },
      "source": [
        "## 1. 要点まとめ\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0Inq78eEX6q"
      },
      "source": [
        "### 過学習\n",
        "\n",
        "学習の結果としての訓練誤差は、学習が進むにつれて小さくなる。<br>\n",
        "対して、予測の結果であるテスト誤差が、学習が進んでも小さくならない、または、大きくなってしまう現象。\n",
        "\n",
        "学習の対象である訓練データに過度に適合し、未知のデータの予測に役立たない。\n",
        "\n",
        "原因と、本章での対応方法は以下の通り。\n",
        "\n",
        "| 原因 | 対応方法 |\n",
        "| -- | -- |\n",
        "| パラメータの値が適切ではない<br>入力値に対して NN が大き過ぎる | L1、L2正則化でパラメータの値を制約 |\n",
        "| ノードが多い | ドロップアウトでノードの数を制約 |\n",
        "\n",
        "- NOTE: ドロップアウトは、一時的に削除するだけなのでは?\n",
        "- NOTE: パラメータの数が多いことと、層数が原因の場合の対応方法は?<br>\n",
        "  または、対応はそれぞれの数を減らすこととして、原因であることを検知する方法は?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJ_YDWdeEcR6"
      },
      "source": [
        "### 正則化\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwUpmyb3Dxfx"
      },
      "source": [
        "#### Weight decay ( 荷重減衰 )\n",
        "\n",
        "重みのばらつき ( 学習の成果 ) を維持しつつ、過学習原因となる大き過ぎる重みを抑制すること。\n",
        "\n",
        "講義「機械学習」/「2. 非線形回帰モデル」にも登場した、 L1正則化、L2正則化を使用する。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZ6aUD_ZDrn-"
      },
      "source": [
        "#### L1 、 L2 正則化\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltIDNXJIEyi9"
      },
      "source": [
        "##### 数式\n",
        "\n",
        "誤差関数に、正則化項(罰則化項)として、 p ノルムを加える。\n",
        "\n",
        "$$\n",
        "E_n(\\textbf{w}) + \\frac{1}{p} \\lambda \\Vert x \\Vert_p\n",
        "$$\n",
        "\n",
        "p ノルムの計算は、以下の通り。\n",
        "\n",
        "$$\n",
        "\\Vert x \\Vert_p = (\n",
        "  \\vert x_1 \\vert^p + \\ldots + \\vert x_n \\vert^p\n",
        ")^{\\frac{1}{p}}\n",
        "$$\n",
        "\n",
        "- p = 1 の場合、 L1 ノルムを使った L1 正則化 ( Lasso 回帰、マンハッタン距離を使用 )\n",
        "- p = 2 の場合、 L2 ノルムを使った L2 正則化 ( Ridge 回帰、ユークリッド距離を使用 )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RElAY4Ubg37A"
      },
      "source": [
        "##### NN の中での適用箇所\n",
        "\n",
        "p ノルムは誤差関数に適用する。(再掲)\n",
        "\n",
        "$$\n",
        "E_n(\\textbf{w}) + \\frac{1}{p} \\lambda \\Vert x \\Vert_p\n",
        "$$\n",
        "\n",
        "p ノルムは、各層の重みから計算する。 ( 例として、2層 )\n",
        "\n",
        "$$\n",
        "\\Vert x \\Vert_p = \\Vert \\textbf{W}^{(1)} \\Vert_p\n",
        "+\n",
        "\\Vert \\textbf{W}^{(2)} \\Vert_p\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\Vert \\textbf{W}^{(1)} \\Vert_p = (\n",
        "  \\vert \\textbf{W}^{(1)}_1 \\vert^p + \\ldots + \\vert \\textbf{W}^{(1)}_n \\vert^p\n",
        ")^{\\frac{1}{p}}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\Vert \\textbf{W}^{(2)} \\Vert_p = (\n",
        "  \\vert \\textbf{W}^{(2)}_1 \\vert^p + \\ldots + \\vert \\textbf{W}^{(2)}_n \\vert^p\n",
        ")^{\\frac{1}{p}}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqJL15eQjBgQ"
      },
      "source": [
        "##### 対応するコード\n",
        "\n",
        "`2_5_overfiting.ipynb` の、 `## weight decay` の下のセル。\n",
        "\n",
        "`### L2` の場合、\n",
        "\n",
        "ノルムの計算は、\n",
        "\n",
        "```python\n",
        "    for idx in range(1, hidden_layer_num+1):\n",
        "        ...\n",
        "        weight_decay += 0.5 * weight_decay_lambda * np.sqrt(np.sum(network.params['W' + str(idx)] ** 2))\n",
        "```\n",
        "\n",
        "誤差関数への適用は、\n",
        "\n",
        "```python\n",
        "    loss = network.loss(x_batch, d_batch) + weight_decay\n",
        "```\n",
        "\n",
        "`### L1` の場合、ノルムの計算が以下の様に変わる。重みを2乗しない。\n",
        "\n",
        "```python\n",
        "    for idx in range(1, hidden_layer_num+1):\n",
        "        ...\n",
        "        weight_decay += weight_decay_lambda * np.sum(np.abs(network.params['W' + str(idx)]))\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqzhcrTUE1XV"
      },
      "source": [
        "##### L1 、 L2 正則化の特徴\n",
        "\n",
        "以下の通り。\n",
        "\n",
        "| 正則化の種類 | 推定量 | 正則化(罰則)項 | 誤差の等高線グラフとの接点 | 重みの推定 |\n",
        "| -- | -- | -- | -- | -- |\n",
        "| L1 正則化 | Lasso | L1 ノルム | $w_0 + w_1 \\leq 1$: 四角の頂点と接しやすい | スハース推定 ( 0 の要素の割合 ) |\n",
        "| L2 正則化 | Ridge | L2 ノルム | $w_0^2 + w_1^2 \\leq 1$: 円の接線と接する | 縮小推定 ( 要素を 0 に近づける ) |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJvZAVfoDheH"
      },
      "source": [
        "#### ドロップアウト\n",
        "\n",
        "ランダムにノードを削除し、データを変化させずに異なるモデルを学習していると NN に解釈させる手法。\n",
        "\n",
        "- NOTE: page. 69 のデータ拡張は、このドロップアウトとはまた別の方法。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGdD3vCXdyoW"
      },
      "source": [
        "##### コード\n",
        "\n",
        "`lesson_2/multi_layer_net.py` の、 `class MultiLayerNet` の `__init__` で、\n",
        "ドロップアウトする割合を設定して、ドロップアウトの層を追加している。\n",
        "\n",
        "```python\n",
        "            if self.use_dropout:\n",
        "                self.layers['Dropout' + str(idx)] = layers.Dropout(dropout_ratio)\n",
        "```\n",
        "\n",
        "ドロップアウトな設定は、\n",
        "`common/optimizer.py` の、 `class Dropout` に設定される。\n",
        "\n",
        "```python\n",
        "    def forward(self, x, train_flg=True):\n",
        "        if train_flg:\n",
        "            self.mask = np.random.rand(*x.shape) > self.dropout_ratio\n",
        "            return x * self.mask\n",
        "        else:\n",
        "            return x * (1.0 - self.dropout_ratio)\n",
        "\n",
        "    def backward(self, dout):\n",
        "        return dout * self.mask\n",
        "```\n",
        "\n",
        "学習の、前伝播にドロップアウトするノードをマスクで決定して、使用するノードを抑制する。\n",
        "学習の逆伝播 ( 誤差関数計算時 ) と、予測時にも前伝播の時に決めたマスクでノードを抑制する。\n",
        "\n",
        "マスクは、イテレーションを試行する度に変わるので、それに合わせてドロップアウトされるノードが変わる。よって、同じデータで異なるモデルを学習させる効果がある。\n",
        "\n",
        "イテレーションの試行をしているのは、 `lesson_2/2_5_overfiting.ipynb` の、\n",
        "`## Dropout` から 2つ下のあるセル。\n",
        "\n",
        "```python\n",
        "for i in range(iters_num):\n",
        "...\n",
        "    loss = network.loss(x_batch, d_batch)\n",
        "...\n",
        "    \n",
        "    if (i+1) % plot_interval == 0:\n",
        "        accr_train = network.accuracy(x_train, d_train)\n",
        "        accr_test = network.accuracy(x_test, d_test)\n",
        "...\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dn0nkaSx8-Em"
      },
      "source": [
        "## 2. 確認テスト\n",
        "\n",
        "以降の \"page. \" は、講義資料のページの番号です。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKKJYC61aH06"
      },
      "source": [
        "### Page. 59\n",
        "\n",
        "以下を最小化する回帰をリッジ回帰。\n",
        "\n",
        "$$\n",
        "E(\\textbf{w}) = \\Vert \\textbf{y} - \\hat{\\textbf{X}}\\textbf{w} \\Vert^2\n",
        "+\n",
        "\\lambda \\Vert \\textbf{w} \\Vert^2\n",
        "$$\n",
        "\n",
        "$ \\lambda = 0 $ とすると、\n",
        "\n",
        "$$\n",
        "E(\\textbf{w}) = \\Vert \\textbf{y} - \\hat{\\textbf{X}}\\textbf{w} \\Vert^2\n",
        "$$\n",
        "\n",
        "として、二乗和誤差を最小化する線形回帰となる。\n",
        "しかし、解答の \"(b)\" は、「非線形回帰」。選択できなかった。\n",
        "\n",
        "正則化項について、\n",
        "\n",
        "$$\n",
        "\\lambda \\Vert \\textbf{w} \\Vert^2\n",
        "$$\n",
        "\n",
        "$ \\lambda $ を大きな値にすると、全体を最小化しなくてはならないから、\n",
        "$ \\textbf{w} $ は小さくならなくてはならない。<br>\n",
        "また、 \"限りなく 0 に近づく\" であって、 0 にはならない。\n",
        "page. 64 のグラフの、左側 ( Ridge 推定量 ) の通り、誤差の等高線と制約の領域(円)との接線が、縦軸、横軸の位置にならない、つまり、縦方向、横方向のパラメータが 0 にならないため。\n",
        "( 右側の Lasso では制約の四角形の頂点で接するので、グラフの例では、縦軸に接して、横方向のパラメータが 0 になりえる。 )<br>\n",
        "よって、 \"(a)\" を選択。\n",
        "\n",
        "- NOTE: [超入門！リッジ回帰・Lasso回帰・Elastic Netの基本と特徴をサクッと理解！ | AIZINE（エーアイジン）](https://aizine.ai/ridge-lasso-elasticnet/)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9qnZ1q7EqAF"
      },
      "source": [
        "### Page. 64\n",
        "\n",
        "Lasso 推定量で、スパース推定である、右側のグラフである。<br>\n",
        "誤差の等高線が、制約を表す四角の頂点と接しやすい\n",
        "\n",
        "- NOTE: [PRMLの線形回帰モデル（線形基底関数モデル）](https://www.slideshare.net/yasunoriozaki12/prml-29439402)\n"
      ]
    }
  ]
}