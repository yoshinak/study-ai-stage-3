{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1-4-gradient-descent.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOQ2c6S3Iy/YGR8gtCkeOJc"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Adnxs5kDc2pd"
      },
      "source": [
        "# Section 4: 勾配降下法\n",
        "\n",
        "本書は、「深層学習前編（day1）レポート」の、「Section 4: 勾配降下法」についてのものです。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_jMHYWudCh3"
      },
      "source": [
        "## 1. 要点まとめ\n",
        "\n",
        "勾配降下法の種類は、以下の通り。\n",
        "\n",
        "- 勾配降下法 ( 最急降下法 )\n",
        "- 確率的勾配降下法\n",
        "- ミニバッチ勾配降下法\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kl7R1M_RUTLG"
      },
      "source": [
        "全結合 NN において、勾配降下法を適用する目的は、\n",
        "誤差 $ E(\\textbf{w}) $ を最小化するパラメータ $ \\textbf{w} $ を発見するため。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmpbJt_XJSQL"
      },
      "source": [
        "### 勾配降下法\n",
        "\n",
        "数式は以下の通り。\n",
        "\n",
        "$$\n",
        "\\textbf{w}^{(t+1)} = \\textbf{w}^{(t)} - \\epsilon \\nabla E\n",
        "$$\n",
        "\n",
        "( $ \\epsilon $ : 学習率 )\n",
        "\n",
        "$$\n",
        "\\nabla E = \\frac{\\partial{E}}{\\partial{\\textbf{w}}}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "\\frac{\\partial{E}}{\\partial{w_1}}\n",
        "\\cdots\n",
        "\\frac{\\partial{E}}{\\partial{w_M}}\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "<!-- M は層の数\n",
        "$$\n",
        "(\n",
        "  \\textbf{w} = \\begin{bmatrix}\n",
        "    w_1 \\\\\n",
        "    \\vdots \\\\\n",
        "    w_M\n",
        "  \\end{bmatrix}\n",
        ")\n",
        "$$\n",
        "-->\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63jq7ay1Xn2R"
      },
      "source": [
        "#### 学習率: $ \\epsilon $\n",
        "\n",
        "適切な大きさに調整が必要。\n",
        "\n",
        "- 大きすぎる場合<br>\n",
        "最小値にたどり着かずに発散。\n",
        "\n",
        "- 小さすぎる場合<br>\n",
        "発散はしないが、学習時間が長い。(非効率)<br>\n",
        "また、最小値に至る前の、部分的な局所解に収束してしまう。<br>\n",
        "( 学習率がある程度大きい場合、部分的な局所解である極小値を無視して、大域的な局所解に向かって収束させることができる )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0v7S_j7OYKH5"
      },
      "source": [
        "#### 学習率の決定、収束性向上のためのアルゴリズム\n",
        "\n",
        "- Momentum<br>\n",
        "Stage 2 機械学習のステージテストで出題されたもの。\n",
        "- AdaGrad\n",
        "- AdaDelta\n",
        "- Adam<br>\n",
        "Momentum に学習率の調整を加えたもの。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZv51wTvZy9Q"
      },
      "source": [
        "#### 修正値の適用\n",
        "\n",
        "勾配降下法によって、重み $ \\textbf{W} $ と パイアス $ \\textbf{b} $ を更新して、\n",
        "再度、学習の試行をする。試行の単位を、エポックと言う。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cv8noVvgJM-z"
      },
      "source": [
        "### 確率的勾配降下法 ( SGD )\n",
        "\n",
        "勾配降下法では、全サンプルに対して計算して、平均誤差を求める。\n",
        "対して、確率的勾配降下法ではランダムに抽出したサンプルから誤差を求める。\n",
        "\n",
        "勾配降下法\n",
        "\n",
        "$$\n",
        "\\textbf{w}^{(t+1)} = \\textbf{w}^{(t)} - \\epsilon \\nabla E\n",
        "$$\n",
        "\n",
        "に対して、\n",
        "\n",
        "$$\n",
        "\\textbf{w}^{(t+1)} = \\textbf{w}^{(t)} - \\epsilon \\nabla E_n\n",
        "$$\n",
        "\n",
        "( $ E_n $ : あるインデックスの訓練データにおける誤差 )\n",
        "\n",
        "計算の効率化と、オンラインへ適用できることがメリット。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvzH5z0DbT9M"
      },
      "source": [
        "\n",
        "学習に使う全データを揃えてから学習を行うことを、バッチ学習と言う。\n",
        "対して、オンライン学習は入手したデータを都度、学習することを言う。\n",
        "徐々に集まったデータ、または、リアルタイムに発生したデータを使って学習する。\n",
        "\n",
        "また、バッチ学習に対して、少ないメモリ量で学習をすることができる。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGz9EUzwJELy"
      },
      "source": [
        "### ミニバッチ勾配降下法\n",
        "\n",
        "ランダムに分割したデータの **集合** (複数のサンプルからなるミニバッチ)\n",
        "$ D_t $ に属するサンプルの平均誤差を求める。\n",
        "\n",
        "$$\n",
        "\\textbf{w}^{(t+1)} = \\textbf{w}^{(t)} - \\epsilon \\nabla E_t \\\\\n",
        "E_t = \\frac{1}{N_t}\n",
        "\\sum_{n \\in D_t} E_n \\\\\n",
        "N_t = \\vert D_t \\vert\n",
        "$$\n",
        "\n",
        "ミニバッチの単位で並列化し、確率的勾配降下法のメリットを損なわずに、計算資源を有効活用し、学習時間を速めることができる。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-l34CxoI-5L"
      },
      "source": [
        "### 誤差勾配の計算\n",
        "\n",
        "数値微分をプログラムで計算するには?<br>\n",
        "微小な数値 ( $ h $ ) を使って計算。( $ \\lim_{\\delta h \\to 0} E_w $ )\n",
        "\n",
        "$$\n",
        "\\frac{\\nabla{E}}{\\nabla{w_m}}\n",
        "\\approx\n",
        "\\frac{\n",
        "  E (w_m + h) - E (w_m - h)\n",
        "}\n",
        "{2h}\n",
        "$$\n",
        "\n",
        "この計算では、順伝播の計算を繰り返し計算する必要があるため、負荷が大きい。<br>\n",
        "よって、次章の 「Section 5: 誤差逆伝播法」 を使用する。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dn0nkaSx8-Em"
      },
      "source": [
        "## 2. 確認テスト\n",
        "\n",
        "以降の \"page. \" は、講義資料のページの番号です。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BU8McafBU3rt"
      },
      "source": [
        "### page. 50\n",
        "\n",
        "`1_3_stochastic_gradient_descent.ipynb` において、以下の箇所。<br>\n",
        "(ここでは、確率的勾配降下法を使用)\n",
        "\n",
        "```python\n",
        "# 勾配降下の繰り返し\n",
        "for dataset in random_datasets:\n",
        "    x, d = dataset['x'], dataset['d']\n",
        "    z1, y = forward(network, x)\n",
        "    grad = backward(x, d, z1, y)\n",
        "    # パラメータに勾配適用\n",
        "    for key in ('W1', 'W2', 'b1', 'b2'):\n",
        "        network[key]  -= learning_rate * grad[key]\n",
        "\n",
        "    # 誤差\n",
        "    loss = functions.mean_squared_error(d, y)\n",
        "    losses.append(loss)\n",
        "\n",
        "```\n",
        "\n",
        "上記の、 `network[key]  -= learning_rate * grad[key]` が、<br>\n",
        "$$\n",
        "\\textbf{w}^{(t+1)} = \\textbf{w}^{(t)} - \\epsilon \\nabla E\n",
        "$$\n",
        "にあたる。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRCUl0I2f_67"
      },
      "source": [
        "### page. 59\n",
        "\n",
        "オンライン学習とは、入手したデータを都度、学習することを言う。\n",
        "徐々に集まったデータ、または、リアルタイムに発生したデータを使って学習する。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dfHNQDedVao"
      },
      "source": [
        "### page. 62\n",
        "\n",
        "新たな重み $ \\textbf{w}^{(t+1)} $ 1つ前の勾配を使って求める。\n",
        "\n",
        "- 1回目の試行 ( $ t $ ) 後<br>\n",
        "$$ \\textbf{w}^{(t+1)} = \\textbf{w}^{(t)} - \\epsilon \\nabla E_t $$<br>\n",
        "2回目に、 $ \\textbf{w}^{(t+1)} $ を使って試行する。\n",
        "\n",
        "- 2回目の試行 ( $ t + 1 $ ) 後<br>\n",
        "$$ \\textbf{w}^{(t+2)} = \\textbf{w}^{(t+1)} - \\epsilon \\nabla E_{t+1} $$<br>\n",
        "3回目に、 $ \\textbf{w}^{(t+2)} $ を使って試行する。\n",
        "\n",
        "- n回目の試行 ( $ t + (n - 1) $ ) 後<br>\n",
        "$$ \\textbf{w}^{(t+n)} = \\textbf{w}^{(t+(n-1))} - \\epsilon \\nabla E_{t+(n-1)} $$<br>\n",
        "n回目に、 $ \\textbf{w}^{(t+n)} $ を使って試行する。<br>\n",
        "もし、 $ E_{t+(n-1)}  $ が 0 ないし、充分に小さい値の場合は、<br>\n",
        "または、試行上限回数に達した場合学習終了。\n"
      ]
    }
  ]
}